{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download vectors -- DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Using gensim, extract embeddings of words in Table 1 of RG65 that also appeared in\n",
    "# the set W from the earlier exericse, i.e., the pairs of words should be identical in all analyses.\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_W2V = KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jreneewatson/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_multiprocessing_helpers.py:38: UserWarning: [Errno 12] Cannot allocate memory.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=0.7878, p=3.579379128582004e-13\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Calculate cosine distance between each pair of word embeddings you have extracted,\n",
    "# and report the Pearson correlation between word2vec-based and human similarities. [1 point]\n",
    "# Comment on this value in comparison to those from LSA and word-context vectors from analyses\n",
    "# in the earlier exercise. [1 point]\n",
    "\n",
    "import common\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def get_sim(embedding_dict, word_pairs):\n",
    "    result = []\n",
    "    for w1, w2 in word_pairs:\n",
    "        w1_vec = embedding_dict[w1]\n",
    "        w2_vec = embedding_dict[w2]\n",
    "        result.append(cosine_similarity([w1_vec], [w2_vec])[0][0])\n",
    "    return result\n",
    "\n",
    "\n",
    "# Load LSA embeddings from exercise.\n",
    "M2300 = common.load_embedding_dict(\"data/M2300.pickle\")\n",
    "\n",
    "# Filter RG65 word pairs, based on what words have LSA embeddings\n",
    "# (so results are comparable).\n",
    "rg65_words, rg65_word_pairs = common.load_rg65()\n",
    "P = [(w1, w2) for w1, w2 in rg65_word_pairs.keys()\n",
    "     if w1 in M2300 and w2 in M2300]\n",
    "S = [rg65_word_pairs[word_pair] for word_pair in P]\n",
    "\n",
    "# Compute cosine similarity for word pairs, based on word2vec embeddigs.\n",
    "SW2V = get_sim(model_W2V, P)\n",
    "\n",
    "# Compare w2v similarity to human annotations.\n",
    "r, p = pearsonr(SW2V, S)\n",
    "print(f\"r={r:.4f}, p={p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Perform the analogy test based on data here (or as provided) with the pre-trained\n",
    "# word2vec embeddings. Report the accuracy on the semantic analogy test and the syntactic\n",
    "# analogy test (see Note below).\n",
    "\n",
    "# Repeat the analysis with LSA vectors (300 dimensions) from the earlier exercise, and commment\n",
    "# on the results in comparison to those from word2vec. [1 point]\n",
    "\n",
    "# Note: It is expected that the number of entries you could test with LSA\n",
    "# would be smaller than that based on word2vec. For a fair comparison, you should consider\n",
    "# reporting model accuracies based on the small test set, for both word2vec and LSA.\n",
    "\n",
    "import common\n",
    "            \n",
    "\n",
    "M2300 = common.load_embedding_dict(\"data/M2300.pickle\")\n",
    "analogy_task = common.load_analogy_task(vocab=set(M2300.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Berlin\" in M2300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Suggest a way to improve the existing set of vector-based models in capturing word\n",
    "# similarities in general, and provide justifications for your suggestion. [2 points]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
