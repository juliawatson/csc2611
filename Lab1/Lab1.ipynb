{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Synchronic word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Download vectors\n",
    "\n",
    "**Step 2**: Using gensim, extract embeddings of words in Table 1 of RG65 that also appeared in the set W from the earlier exericse, i.e., the pairs of words should be identical in all analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_W2V = KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Calculate cosine distance between each pair of word embeddings you have extracted, and report the Pearson correlation between word2vec-based and human similarities. [1 point] Comment on this value in comparison to those from LSA and word-context vectors from analyses in the earlier exercise. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jreneewatson/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_multiprocessing_helpers.py:38: UserWarning: [Errno 12] Cannot allocate memory.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=0.7878, p=3.579379128582004e-13\n"
     ]
    }
   ],
   "source": [
    "import common\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def get_sim(embedding_dict, word_pairs):\n",
    "    result = []\n",
    "    for w1, w2 in word_pairs:\n",
    "        w1_vec = embedding_dict[w1]\n",
    "        w2_vec = embedding_dict[w2]\n",
    "        result.append(cosine_similarity([w1_vec], [w2_vec])[0][0])\n",
    "    return result\n",
    "\n",
    "\n",
    "# Load LSA embeddings from exercise.\n",
    "M2300 = common.load_embedding_dict(\"data/M2300.pickle\")\n",
    "\n",
    "# Filter RG65 word pairs, based on what words have LSA embeddings\n",
    "# (so results are comparable).\n",
    "rg65_words, rg65_word_pairs = common.load_rg65()\n",
    "P = [(w1, w2) for w1, w2 in rg65_word_pairs.keys()\n",
    "     if w1 in M2300 and w2 in M2300]\n",
    "S = [rg65_word_pairs[word_pair] for word_pair in P]\n",
    "\n",
    "# Compute cosine similarity for word pairs, based on word2vec embeddigs.\n",
    "SW2V = get_sim(model_W2V, P)\n",
    "\n",
    "# Compare w2v similarity to human annotations.\n",
    "r, p = pearsonr(SW2V, S)\n",
    "print(f\"r={r:.4f}, p={p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Perform the analogy test based on data here (or as provided) with the pre-trained word2vec embeddings. Report the accuracy on the semantic analogy test and the syntactic analogy test (see Note below).\n",
    "\n",
    "Repeat the analysis with LSA vectors (300 dimensions) from the earlier exercise, and commment on the results in comparison to those from word2vec. [1 point]\n",
    "\n",
    "Note: It is expected that the number of entries you could test with LSA would be smaller than that based on word2vec. For a fair comparison, you should consider reporting model accuracies based on the small test set, for both word2vec and LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def run_analogy_task(embedding_dict, analogy_dataset):\n",
    "    words = list(embedding_dict.keys())\n",
    "    embedding_matrix = np.array(\n",
    "        [embedding_dict[i] for i, word in words])\n",
    "    correct = 0\n",
    "    for a, b, c, d_true in analogy_datset:\n",
    "        d_pred_vector = embedding_dict[a] - embedding_dict[b] + embedding_dict[c]\n",
    "        \n",
    "        sim = cosine_similarity([d_pred_vector], embedding_matrix)[0]\n",
    "        d_rankings = np.argsort(-sim)[:4]\n",
    "        d_pred = [words[item] for item in d_rankings\n",
    "                  if words[item] not in {a, b, c}][0]\n",
    "        \n",
    "        if d_pred == d_true:\n",
    "            correct += 1\n",
    "        print(a, b, c, d_true, \"\\t\", d_pred)\n",
    "    accuracy = correct / len(analogy_dataset)\n",
    "    print(f\"accuracy = {:.4f}\")\n",
    "\n",
    "\n",
    "M2300 = common.load_embedding_dict(\"data/M2300.pickle\")\n",
    "analogy_dataset = common.load_analogy_task(vocab=set(M2300.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95538926 0.5531201  0.94072087 0.81468817]\n",
      "[0 2 3 1]\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "# Delete me once you get run_analogy_task working\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "words = [\"a\", \"b\", \"c\", \"d\"]\n",
    "\n",
    "embedding_matrix = np.array([[0.1, 0.2, 0.3], [0.3, 0.2, 0.1], [0, 0, 4], [1, 1, 1]])\n",
    "d_pred_vector = np.array([0.1, 0.15, 0.5])\n",
    "vocab = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3}\n",
    "\n",
    "a, b, c = \"a\", \"b\", \"c\"\n",
    "\n",
    "\n",
    "\n",
    "sim = cosine_similarity([d_pred_vector], embedding_matrix)[0]\n",
    "print(sim)\n",
    "d_rankings = np.argsort(-sim)[:4]\n",
    "print(d_rankings)\n",
    "d_pred = [words[item] for item in d_rankings\n",
    "          if words[item] not in {a, b, c}][0]\n",
    "print(d_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Suggest a way to improve the existing set of vector-based models in capturing word similarities in general, and provide justifications for your suggestion. [2 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**. Download the diachronic word2vec embeddings from the course syllabus page. These embeddings capture historical usage of a small subset of English words over the past century.\n",
    "\n",
    "**Step 2**. Propose three different methods for measuring degree of semantic change for individual words and report the top 20 most and least changing words in table(s) from each measure. Measure the intercorrelations (of semantic change in all words, given the embeddings from Step 1) among the three methods you have proposed and summarize the Pearson correlations in a 3-by-3 table. [3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**. Propose and justify a procedure for evaluating the accuracy of the methods you have proposed in Step 2, and then evaluate the three methods following this proposed procedure and report Pearson correlations or relevant test statistics. [2 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**. Extract the top 3 changing words using the best method from Steps 2 and 3. Propose and implement a simple way of detecting the point(s) of semantic change in each word based on its diachronic embedding time courseâ€”visualize the time course and the detected change point(s). [3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
