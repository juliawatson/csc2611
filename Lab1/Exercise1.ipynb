{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 2. Write test code for ppmi function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Import NLTK in Python: http://www.nltk.org/.\n",
    "# Download the Brown Corpus http://www.nltk.org/book/ch02.html for analyses below.\n",
    "\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Extract the 5000 most common English words (denoted by W) based on unigram\n",
    "# frequencies in the Brown corpus. Report the 5 most and least common words you have found\n",
    "# in the 5000 words. Update W by adding n words where n is the set of words in Table 1\n",
    "# of RG65 that were not included in the top 5000 words from the Brown corpus. Denote the\n",
    "# total number of words in W as |W|.\n",
    "\n",
    "from collections import Counter\n",
    "import csv\n",
    "import common\n",
    "\n",
    "\n",
    "def get_most_common_words(n):\n",
    "    unigram_counter = Counter()\n",
    "    for sent in brown.sents():\n",
    "        unigram_counter.update(sent)\n",
    "    most_common_words = set([item[0] for item in unigram_counter.most_common(n)])\n",
    "    return most_common_words\n",
    "\n",
    "\n",
    "most_common_5000 = get_most_common_words(5000)\n",
    "rg65_words, rg65_word_pairs = common.load_rg65()\n",
    "w_vocab = most_common_5000.union(rg65_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1161191it [00:04, 277003.98it/s]\n",
      "1161191it [00:05, 212287.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 3. Construct a word-context vector model (denoted by M1) by collecting bigram counts\n",
    "# for words in W. The output should be a |W|×|W| matrix (consider using sparse matrices\n",
    "# for better efficiency), where each row is a word in W, and each column is a context in W\n",
    "# that precedes row words in sentences. For example, if the phrase taxi driver appears 5 times\n",
    "# in the entire corpus, then row taxi and column driver should have a value of 5 in the matrix.\n",
    "\n",
    "import scipy.sparse\n",
    "from nltk.util import bigrams\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_w_matrix(w_vocab):\n",
    "    \"\"\"\n",
    "    Each row is a word and each column is a context.\n",
    "    \"\"\"\n",
    "    # Identify which words and contexts to include (don't want any\n",
    "    # zero rows or zero columns).\n",
    "    w_vocab_contexts = set()\n",
    "    w_vocab_words = set()\n",
    "    for context, word in tqdm(bigrams(brown.words())):\n",
    "        if context not in w_vocab or word not in w_vocab:\n",
    "            continue\n",
    "        w_vocab_contexts.add(context)\n",
    "        w_vocab_words.add(word)    \n",
    "    word_to_index = {word: i for i, word in enumerate(list(w_vocab_words))}\n",
    "    context_to_index = {word: i for i, word in enumerate(list(w_vocab_contexts))}\n",
    "    w_matrix = np.zeros((len(word_to_index), len(context_to_index)))\n",
    "    \n",
    "    \n",
    "    for context, word in tqdm(bigrams(brown.words())):\n",
    "        if word not in word_to_index or context not in context_to_index:\n",
    "            continue\n",
    "        w_matrix[word_to_index[word], context_to_index[context]] += 1\n",
    "    return word_to_index, w_matrix\n",
    "\n",
    "\n",
    "word_to_index, M1 = get_w_matrix(w_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Compute positive pointwise mutual information on M1. Denote this model as M1+.\n",
    "\n",
    "def get_ppmi_matrix(count_matrix):\n",
    "    count_matrix = count_matrix / np.sum(count_matrix)\n",
    "    p_word = np.sum(count_matrix, axis=1) / np.sum(count_matrix)    \n",
    "    p_context = np.sum(count_matrix, axis=0) / np.sum(count_matrix)\n",
    "\n",
    "    count_matrix = np.divide(count_matrix, p_context, where=count_matrix!=0)\n",
    "    count_matrix = np.divide(count_matrix.T, p_word, where=count_matrix.T!=0).T\n",
    "    \n",
    "    count_matrix = np.log2(count_matrix, where=count_matrix!=0)\n",
    "    count_matrix[count_matrix < 0] = 0\n",
    "    return count_matrix\n",
    "\n",
    "\n",
    "M1_plus = get_ppmi_matrix(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Construct a latent semantic model (denoted by M2) by applying principal components\n",
    "# analysis to M1+. The output should return 3 matrices, with different truncated\n",
    "# dimenions at 10 (or a |W|×10 matrix, denoted by M210), 100 (M2100), and 300 (M2300).\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "    \n",
    "M210 = PCA(n_components=10).fit_transform(M1_plus)\n",
    "M2100 = PCA(n_components=100).fit_transform(M1_plus)\n",
    "M2300 = PCA(n_components=300).fit_transform(M1_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6. Find all pairs of words in Table 1 of RG65 that are also available in W. Denote\n",
    "# these pairs as P. Record the human-judged similarities of these word pairs from the table\n",
    "# and denote similarity values as S.\n",
    "\n",
    "P = [(w1, w2) for w1, w2 in rg65_word_pairs.keys()\n",
    "     if w1 in word_to_index and w2 in word_to_index]\n",
    "S = [rg65_word_pairs[word_pair] for word_pair in P]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7. Perform the following calculations on each of these models M1, M1+, M210, M2100,\n",
    "# M2300, separately: Calculate cosine similarity between each pair of words in P, based on the\n",
    "# constructed word vectors. Record model-predicted similarities: SM1, SM210 , SM2100 , SM2300 .\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_sim(word_matrix, word_to_index, word_pairs):\n",
    "    result = []\n",
    "    for w1, w2 in word_pairs:\n",
    "        w1_vec = word_matrix[word_to_index[w1]]\n",
    "        w2_vec = word_matrix[word_to_index[w2]]\n",
    "        result.append(cosine_similarity([w1_vec], [w2_vec])[0][0])\n",
    "    return result\n",
    "\n",
    "\n",
    "SM1 = get_sim(M1, word_to_index, P)\n",
    "SM1_plus = get_sim(M1_plus, word_to_index, P)\n",
    "SM210 = get_sim(M210, word_to_index, P)\n",
    "SM2100 = get_sim(M2100, word_to_index, P)\n",
    "SM2300 = get_sim(M2300, word_to_index, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1, r=0.0895, p=0.5078\n",
      "M1_plus, r=0.2759, p=0.0378\n",
      "M210, r=0.1004, p=0.4573\n",
      "M2100, r=0.2700, p=0.0423\n",
      "M2300, r=0.3085, p=0.0196\n"
     ]
    }
   ],
   "source": [
    "# Step 8. Report Pearson correlation between S and each of the model-predicted similarities.\n",
    "# Create a GitHub repository that implements all of your analyses; you will need this repo for\n",
    "# the next lab.\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "name_to_sim = {\n",
    "    \"M1\": SM1,\n",
    "    \"M1_plus\": SM1_plus,\n",
    "    \"M210\": SM210,\n",
    "    \"M2100\": SM2100,\n",
    "    \"M2300\": SM2300,  \n",
    "}\n",
    "\n",
    "for name, sim in name_to_sim.items():\n",
    "    r, p = pearsonr(sim, S)\n",
    "    print(f\"{name}, r={r:.4f}, p={p:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save word embeddings:\n",
    "import common\n",
    "\n",
    "common.write_embedding_dict(\"data/M2300.pickle\", M2300, word_to_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
